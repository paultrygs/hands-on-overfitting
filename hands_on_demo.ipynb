{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **polynomial** $p(x)$ is a sum of weighted powers of $x$.\n",
    "\n",
    "The **degree** of $p$ is its highest power.\n",
    "\n",
    "Example:\n",
    "$$\n",
    "1+x\n",
    "$$\n",
    "is a polynomial of degree 1, and\n",
    "$$ \n",
    "-2.1x + 3.4x^3\n",
    "$$\n",
    "is a polynomial of degree 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(-1, 1, 50)\n",
    "\n",
    "polynomial_1 = lambda x: 1 + x\n",
    "polynomial_2 = lambda x: - 2.1 * x  + 3.4 * np.power(x, 3)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1)\n",
    "axs[0].plot(X, polynomial_1(X))\n",
    "axs[0].text(0, 1.25, \"1+x\")\n",
    "axs[1].plot(X, polynomial_2(X))\n",
    "axs[1].text(0.25, -0.15, \"-2.1x + 3.4x^3\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: Notice how polynomials of higher degree can \"wiggle\" more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomials as Machine Learning Models\n",
    "\n",
    "We can attempt to find the weights that provide the best fit for a given training dataset.\n",
    "\n",
    "Example: For a polynomial $a_0 + a_1 x$ of degree 1, this means that we attempt to find weights $a_0$ and $a_1$ that gives the best fit.\n",
    "\n",
    "**Remark**: We will not go into details about how weights of models are calculated. \n",
    "\n",
    "**The main takaway**:\n",
    "- The number of weights in a polynomial model is equal to its degree + 1.\n",
    "- Therefore, higher degrees result in more \"complex\" models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.polynomial_data import get_polynomial_training_data, true_function\n",
    "from src.polynomial_visualization import visualize_model\n",
    "\n",
    "# We import the training data for this demo \n",
    "X_train, y_train = get_polynomial_training_data(30)\n",
    "\n",
    "# plot the data together with the true function from which it is sampled:\n",
    "visualize_model(X_train, y_train, None, true_function=true_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal** Train a polynomial model to learn the underlying trend/pattern/rule in the data. The true function reveals the correct trend/pattern/rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Polynomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.polynomial_visualization import visualize_model\n",
    "from src.polynomial_model import get_polynomial_model\n",
    "from src.polynomial_data import get_polynomial_training_data, true_function\n",
    "\n",
    "\n",
    "# We import the training data for this demo \n",
    "X_train, y_train = get_polynomial_training_data(30)\n",
    "\n",
    "# We initialize an untrained polynomial model of degree 1\n",
    "model = get_polynomial_model(degree=1)\n",
    "\n",
    "# We train the model\n",
    "model.fit(X_train[:, np.newaxis], y_train)\n",
    "\n",
    "# We plot the trained model together with the raw data and the true function/trend\n",
    "visualize_model(X_train, y_train, model=model, true_function=true_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Observation**: We can intuitively see that this model is too simple for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.polynomial_model import get_polynomial_model_with_regularization\n",
    "\"\"\"\n",
    "Exercise:\n",
    "You can press play to interactively train and plot polynomial models for different degrees. \n",
    "\n",
    "a) How does the model with degree=50 perform for X values in between 0.8 and 1?\n",
    "\n",
    "b) How does the model with degree=7 perform for X values in between 0.8 and 1?\n",
    "\n",
    "c) What degree (number of weights) do you think gives the \"best\" fit?\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from ipywidgets import interactive, fixed\n",
    "from src.polynomial_model import get_polynomial_model, get_polynomial_model_with_regularization\n",
    "from src.polynomial_data import get_polynomial_training_data, true_function\n",
    "from src.polynomial_visualization import visualize_model\n",
    "\n",
    "def fit_and_plot_polynomial_from_degree(degree: int, X: np.ndarray, y: np.ndarray, regularization=False):\n",
    "    # We initialize the polynomial model\n",
    "    if regularization:\n",
    "        model = get_polynomial_model_with_regularization(degree=degree)\n",
    "    else:    \n",
    "        model = get_polynomial_model(degree=degree)\n",
    "\n",
    "    # We train the model\n",
    "    model.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # We plot the trained model together with the raw data and the true function/trend\n",
    "    visualize_model(X, y, model=model, true_function=true_function)\n",
    "\n",
    "\n",
    "# We import the training data for this demo \n",
    "X_train, y_train = get_polynomial_training_data(30)\n",
    "\n",
    "\n",
    "w = interactive(\n",
    "    fit_and_plot_polynomial_from_degree,\n",
    "    degree=(1, 50),\n",
    "    X=fixed(X_train),\n",
    "    y=fixed(y_train),\n",
    "    regularization=fixed(False),\n",
    ")\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting\n",
    "\n",
    "Underfitting occurs when the model is too simplistic to learn the underlying pattern or rule in the data. Its typical symptom is poor performance on both the training data and new, unseen data.\n",
    "\n",
    "Overfitting, on the other hand, occurs when the model is too complex and learns too much from the training data, such that it fits to the noise of the training data. Its typical symptom is good performance on the training data but poor performance on new and unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise continued:\n",
    "\n",
    "d) For which degrees do you think the model is \n",
    "    i) underfitting?\n",
    "    ii) Overfitting?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Underfitting and Overfitting\n",
    "\n",
    "In simple models, it's possible to identify underfitting and overfitting by visually inspecting the training and test set performance. However, for more complex models such as visual classification models, it's not easy to visually discern under- and overfitting.\n",
    "\n",
    "**The goal** is to measure these phenomena quantitatively using numerical metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen Test Data\n",
    "\n",
    "The following code includes new and unseen data to evaluate the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.polynomial_data import get_polynomial_test_data, true_function\n",
    "from src.polynomial_visualization import visualize_model\n",
    "\n",
    "# We import some new, unseen test data\n",
    "X_test, y_test = get_polynomial_test_data(number_of_points= 30)\n",
    "\n",
    "# We plot the test data together with the true function from which it is sampled:\n",
    "visualize_model(X_test, y_test, None, true_function=true_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Model Performance\n",
    "\n",
    "There are several methods to evaluate the performance of a trained machine learning model on test data.\n",
    "\n",
    "In this exercise, we will use the Mean Absolute Error (MAE) metric from the scikit-learn metrics library, which is a quantitative measure of how far the predictions are from the actual values. A MAE score of 0 indicates a perfect fit for the model, while a large error score indicates a bad model fit or poor performance.\n",
    "\n",
    "**Disclaimer**: Other metrics may also be useful for measuring a model's performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src.polynomial_model import get_polynomial_model\n",
    "from src.polynomial_data import get_polynomial_training_data, get_polynomial_test_data\n",
    "\n",
    "# We import the training data for this demo \n",
    "X_train, y_train = get_polynomial_training_data(30)\n",
    "\n",
    "# We import some new, unseen test data\n",
    "X_test, y_test = get_polynomial_test_data(number_of_points= 30)\n",
    "\n",
    "# We initialize a polynomial model of degree 7\n",
    "model = get_polynomial_model(degree=7)\n",
    "\n",
    "# We train the model on the original test data\n",
    "model.fit(X_train[:, np.newaxis], y_train)\n",
    "\n",
    "# We calculate predicted y values from the original training data\n",
    "y_predicted_train = model.predict(X_train[:, np.newaxis])\n",
    "\n",
    "# We calculate predicted y values from the new test data\n",
    "y_predicted_test = model.predict(X_test[:, np.newaxis])\n",
    "\n",
    "# We measure the error when predicting on original training data\n",
    "train_error = mean_absolute_error(y_train, y_predicted_train)\n",
    "\n",
    "# We measure the error when predicting on new test data\n",
    "test_error = mean_absolute_error(y_test, y_predicted_test)\n",
    "\n",
    "print(f\"Train error: {train_error}\")\n",
    "print(f\"Test error: {test_error}\")\n",
    "print(f\"Difference: {abs(test_error - train_error)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise:\n",
    "You can press play to plot test and training errors against the model degree, \n",
    "and also visualize trained models interactively, as in the previous exercise.\n",
    "\n",
    "How can you use the relationship between test and train errors to determine \n",
    "\n",
    "a) Underfitting?\n",
    "b) Overfitting?\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src.polynomial_model import get_polynomial_model, fit_and_plot_polynomial_from_degree\n",
    "from src.polynomial_data import get_polynomial_training_data, get_polynomial_test_data\n",
    "\n",
    "\n",
    "# We import the training data for this demo \n",
    "X_train, y_train = get_polynomial_training_data(30)\n",
    "\n",
    "# We import some new, unseen test data\n",
    "X_test, y_test = get_polynomial_test_data(number_of_points= 30)\n",
    "\n",
    "degrees = np.linspace(1, 22).astype(int)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "for degree in degrees:\n",
    "    # We initialize the polynomial model\n",
    "    model = get_polynomial_model(degree=degree)\n",
    "\n",
    "    # We train the model on the original test data\n",
    "    model.fit(X_train[:, np.newaxis], y_train)\n",
    "\n",
    "    # We calculate predicted y values from the original training data\n",
    "    y_predicted_train = model.predict(X_train[:, np.newaxis])\n",
    "\n",
    "    # We calculate predicted y values from the new test data\n",
    "    y_predicted_test = model.predict(X_test[:, np.newaxis])\n",
    "\n",
    "    # We append the train error\n",
    "    train_errors.append(mean_absolute_error(y_train, y_predicted_train))\n",
    "\n",
    "    # We append the test error\n",
    "    test_errors.append(mean_absolute_error(y_test, y_predicted_test))\n",
    "\n",
    "plt.plot(degrees, train_errors, color=\"blue\", label=\"Train error\")\n",
    "plt.plot(degrees, test_errors, color=\"red\", label=\"Test error\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "from ipywidgets import interactive, fixed\n",
    "\n",
    "w = interactive(\n",
    "    fit_and_plot_polynomial_from_degree,\n",
    "    degree=(1, 25),\n",
    "    X=fixed(X_train),\n",
    "    y=fixed(y_train),\n",
    "    regularization=fixed(False),\n",
    ")\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Strategies to Handle Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1: Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise:\n",
    "You can press play to visualize trained models interactively, as in the previous exercises.\n",
    "\n",
    "How can we avoid overfitting with the training data and polynomial models we have used so far?\n",
    "\"\"\"\n",
    "\n",
    "from ipywidgets import interactive, fixed\n",
    "from src.polynomial_data import get_polynomial_training_data\n",
    "from src.polynomial_model import fit_and_plot_polynomial_from_degree\n",
    "\n",
    "# We import the training data for this demo \n",
    "X_train, y_train = get_polynomial_training_data(30)\n",
    "\n",
    "w = interactive(\n",
    "    fit_and_plot_polynomial_from_degree,\n",
    "    degree=(1, 50),\n",
    "    X=fixed(X_train),\n",
    "    y=fixed(y_train),\n",
    "    regularization=fixed(False),\n",
    ")\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: More Training Data\n",
    "\n",
    "**Intuition**: More training data makes it easier to determine more weights!\n",
    "\n",
    "**Disclaimer**: The training data still has to be representative of the trend/pattern/rule we would like to learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise:\n",
    "You can press play to interactively train and plot polynomial models for\n",
    "different degrees and training dataset sizes.\n",
    "\n",
    "a) Fix degree=50 while changing the training dataset size. What do you see?\n",
    "b) How does increasing training dataset size affect overfitting?\n",
    "c) Does the added training data capture the trend/pattern/rule we would like to learn?\n",
    "\"\"\"\n",
    "\n",
    "from ipywidgets import interactive\n",
    "from src.polynomial_data import get_polynomial_training_data\n",
    "from src.polynomial_model import fit_and_plot_polynomial_from_degree\n",
    "\n",
    "def plot_model_from_degree_and_training_size(degree: int, training_data_size: int):\n",
    "    X_train, y_train = get_polynomial_training_data(number_of_points=training_data_size)\n",
    "    fit_and_plot_polynomial_from_degree(\n",
    "        degree=degree,\n",
    "        X=X_train,\n",
    "        y=y_train\n",
    "    )\n",
    "\n",
    "\n",
    "w = interactive(\n",
    "    plot_model_from_degree_and_training_size,\n",
    "    degree=(1, 100),\n",
    "    training_data_size=(3, 250),\n",
    ")\n",
    "\n",
    "w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Data in Real-World ML \n",
    "\n",
    "While it is easy to add more training data in our exercise because it's created from code, it's generally **not** the case for real-world ML applications.\n",
    "\n",
    "**Data Augmentation**:\n",
    "Create more training data from existing training data. This is done by augmenting existing training data, e.g. rotating images in somehting like visual classification.\n",
    "\n",
    "**Key Takeaway**:\n",
    "\n",
    "There are methods for adding more training data from existing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3: Regularization\n",
    "\n",
    "Add rules/constraints to the ML model's weights.\n",
    "\n",
    "We will have a look at how restricting the size of a model's weights affect under- and overfitting.\n",
    "\n",
    "**Disclaimer**:\n",
    "\n",
    "There are many methods for regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise:\n",
    "You can press play to interactively train and plot polynomial models, \n",
    "with regularization, for different degrees.\n",
    "\n",
    "How does the resulting models differ from the ones trained without regularization?\n",
    "\"\"\"\n",
    "\n",
    "from ipywidgets import interactive\n",
    "from src.polynomial_data import get_polynomial_training_data\n",
    "from src.polynomial_model import fit_and_plot_polynomial_from_degree\n",
    "\n",
    "def plot_model_from_degree_with_regularzation(degree: int):\n",
    "    X_train, y_train = get_polynomial_training_data(30)\n",
    "    fit_and_plot_polynomial_from_degree(\n",
    "        degree,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        regularization=True\n",
    "    )\n",
    "\n",
    "\n",
    "w = interactive(\n",
    "    plot_model_from_degree_with_regularzation,\n",
    "    degree=(1, 100),\n",
    ")\n",
    "\n",
    "w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here is a summary of key takeaways from this exercise:\n",
    "\n",
    "- A model that performs well on training data but poorly on new data is overfit.\n",
    "- We can determine overfitting by comparing error/accuracy between training and new data.\n",
    "    - Relatively worse performance on new data implies overfitting.\n",
    "    - There are many methods for measuring accuracy/error!\n",
    "- Three common strategies for addressing overfitting include:\n",
    "    1. Starting with a simple model and then gradually increasing complexity.\n",
    "    2. Adding more training data, provided it captures what we want to learn.\n",
    "    3. Regularization techniques can be employed to manage overfitting.\n",
    "        - There are many methods for regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise:\n",
    "\n",
    "For the working example used in this interactive demo,\n",
    "which strategy do you think gives the best result?\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
